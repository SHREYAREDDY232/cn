import numpy as np
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from gensim.models import Word2Vec
text_data = [
    "This is the first document.",
    "This document is the second document.",
    "And this is the third one.",
    "Is this the first document?",
]
# i) One Hot Encoding
def one_hot_encoding(text_data):
    unique_words = set(" ".join(text_data).split())
    encoded_data = []
    for text in text_data:
        encoded_text = [1 if word in text else 0 for word in unique_words]
        encoded_data.append(encoded_text)
    return np.array(encoded_data)
one_hot_encoded = one_hot_encoding(text_data)
print("One Hot Encoding:")
print(one_hot_encoded)

# ii) Bag of Words (BOW)
vectorizer = CountVectorizer()
bow_features = vectorizer.fit_transform(text_data)
print("\nBag of Words (BOW):")
print(bow_features.toarray())
print(bow_features.toarray())

# iii) n-grams
ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))
ngram_features = ngram_vectorizer.fit_transform(text_data)
print("\nn-grams:")
print(ngram_features.toarray())
tfidf_vectorizer = TfidfVectorizer()
tfidf_features = tfidf_vectorizer.fit_transform(text_data)
print("\nTf-Idf:")
print(tfidf_features.toarray())

# v) Custom features (e.g., length of documents)
custom_features = np.array([[len(doc)] for doc in text_data])
print("\nCustom Features:")
print(custom_features)

# vi) Word2Vec (Word Embedding)
import pandas as pd
from gensim.models import FastText

sentences = [["I", "like", "apples"],
["I", "enjoy", "eating", "fruits"]]
# Training the FastText model
model_fasttext = FastText(sentences, min_count=1, window=5, vector_size=100)
# Accessing word vectors
word_vectors = model_fasttext.wv
# Creating a DataFrame for word vectors
word_vectors_df = pd.DataFrame(word_vectors.vectors, index=word_vectors.index_to_key)
# Displaying the word vectors DataFrame
word_vectors_df.head(10)
